{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive models applied to forecast energy needs in buildings\n",
    "\n",
    "Author : Charles Gaydon\n",
    "\n",
    "Started : 29/10/2017\n",
    "\n",
    "Last Edited : 5/11/2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach\n",
    "\n",
    "\n",
    "### Buildings and data\n",
    "\n",
    "Using Tableau, we saw that we have to predict : \n",
    "\n",
    "- Y1, Y2, Y4, Y5 for Bat 1, Bat 2 and Bat 4 ;\n",
    "- Y1, Y3 and Y5 for Bat 3.\n",
    "\n",
    "Hence we will use to train :\n",
    "Bat 1,2,3,4 for Y1, Y5 ;\n",
    "Bat 1,2,4 for Y2, Y4 ;\n",
    "Bat 3 for Y3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import matplotlib.pyplot as mpl\n",
    "\n",
    "X_train = pd.read_csv(\"./train_set_X.csv\",sep=';')\n",
    "X_test = pd.read_csv(\"./test_set_X.csv\",sep=';')\n",
    "All_X = pd.concat([X_train,X_test])\n",
    "All_Y_train = pd.read_csv(\"./train_set_Y.csv\",sep=';')\n",
    "\n",
    "X_train_size = X_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolation\n",
    "Bat 3 lacks a few data point during a short period of time. \n",
    "And when values are equal to an integer 0, it means that they are missing.\n",
    "We hence choose to interpolate the missing data, which we shall do on X_test as well later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import put_nan\n",
    "All_X.iloc[:,3:] = All_X.iloc[:,3:].applymap(put_nan)\n",
    "All_Y_train.iloc[:,1:] = All_Y_train.iloc[:,1:].applymap(put_nan)\n",
    "All_X.interpolate(inplace=True) #TO CHANGE !\n",
    "#import fancyinput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time transformation\n",
    "\n",
    "Based on this [article](http://www.sciencedirect.com/science/article/pii/S0378778804003032) idea to use hour and date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Time transformation\n",
    "All_X['hour'] = 0\n",
    "All_X['sinHour'] = 0\n",
    "All_X['cosHour'] = 0\n",
    "All_X['sinWeekDay'] = 0\n",
    "All_X['cosWeekDay'] = 0\n",
    "All_X[\"hour\"] = All_X['Time'].apply(lambda x : int(x.split()[1][0:2])) #problem here\n",
    "\n",
    "All_X = All_X.reset_index() #necessary for dayofweek to perform\n",
    "All_X['Time'] = pd.to_datetime(All_X['Time'])\n",
    "All_X[\"weekDay\"] = All_X['Time'].dt.dayofweek\n",
    "\n",
    "All_X['sinHour'] = np.sin(2*np.pi*All_X['hour']/23)\n",
    "All_X['sinWeekDay'] = np.sin(2*np.pi*All_X['weekDay']/6)\n",
    "All_X['cosHour'] = np.cos(2*np.pi*All_X['hour']/23)\n",
    "All_X['cosWeekDay'] = np.cos(2*np.pi*All_X['weekDay']/6)\n",
    "# we could separate Mon-Fri and Sat-Sun because the dynamics are really different, but we assume\n",
    "# that adding info about past points is a better alternative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Id</th>\n",
       "      <th>Id_bat</th>\n",
       "      <th>Time</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>hour</th>\n",
       "      <th>sinHour</th>\n",
       "      <th>cosHour</th>\n",
       "      <th>sinWeekDay</th>\n",
       "      <th>cosWeekDay</th>\n",
       "      <th>weekDay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>1.5</td>\n",
       "      <td>8.1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>380.183</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-01 01:00:00</td>\n",
       "      <td>1.6</td>\n",
       "      <td>8.2</td>\n",
       "      <td>21.9</td>\n",
       "      <td>378.100</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.269797</td>\n",
       "      <td>0.962917</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-01 02:00:00</td>\n",
       "      <td>1.4</td>\n",
       "      <td>7.9</td>\n",
       "      <td>21.9</td>\n",
       "      <td>374.983</td>\n",
       "      <td>120.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.519584</td>\n",
       "      <td>0.854419</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-01 03:00:00</td>\n",
       "      <td>1.6</td>\n",
       "      <td>6.9</td>\n",
       "      <td>21.8</td>\n",
       "      <td>376.017</td>\n",
       "      <td>120.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.730836</td>\n",
       "      <td>0.682553</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-01 04:00:00</td>\n",
       "      <td>1.4</td>\n",
       "      <td>6.3</td>\n",
       "      <td>21.8</td>\n",
       "      <td>376.017</td>\n",
       "      <td>120.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.887885</td>\n",
       "      <td>0.460065</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  Id  Id_bat                Time   x1   x2    x3       x4     x5  \\\n",
       "0      0   0       1 2016-01-01 00:00:00  1.5  8.1  22.0  380.183   20.0   \n",
       "1      1   1       1 2016-01-01 01:00:00  1.6  8.2  21.9  378.100  120.0   \n",
       "2      2   2       1 2016-01-01 02:00:00  1.4  7.9  21.9  374.983  120.0   \n",
       "3      3   3       1 2016-01-01 03:00:00  1.6  6.9  21.8  376.017  120.0   \n",
       "4      4   4       1 2016-01-01 04:00:00  1.4  6.3  21.8  376.017  120.0   \n",
       "\n",
       "   hour   sinHour   cosHour  sinWeekDay  cosWeekDay  weekDay  \n",
       "0     0  0.000000  1.000000   -0.866025        -0.5        4  \n",
       "1     1  0.269797  0.962917   -0.866025        -0.5        4  \n",
       "2     2  0.519584  0.854419   -0.866025        -0.5        4  \n",
       "3     3  0.730836  0.682553   -0.866025        -0.5        4  \n",
       "4     4  0.887885  0.460065   -0.866025        -0.5        4  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "All_X.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time dependency\n",
    "\n",
    "We don't want to use a learning model that is explicitly taking into account past events (e.g. LSTM), instead we add a time dependency by adding sum/peak/value of precedents timestamps\n",
    "\n",
    "##### Values at t-1 and t-2, max and mean from 12 last observations\n",
    "We will get rid of the values observations containing values we cannot get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "factors = ['x1','x2','x3','x4','x5']\n",
    "period = 4\n",
    "for f in factors :\n",
    "    for id in range(1,5) : \n",
    "        All_X[f+'(t-1)'] = 0\n",
    "        All_X[f+'(t-2)'] = 0\n",
    "        All_X[f+'(t-1)'] = All_X[f].shift(1)\n",
    "        All_X[f+'(t-2)'] = All_X[f].shift(2)\n",
    "        All_X[f+'max'+str(period)+'H'] =  All_X[f].rolling(period,min_periods=period).max()\n",
    "        All_X[f+'mean'+str(period)+'H'] =  All_X[f].rolling(period,min_periods=period).mean()\n",
    "        \n",
    "    for p in range(1,period+1):\n",
    "            index = np.where(All_X['Id_bat'] != All_X['Id_bat'].shift(p))[0]\n",
    "            All_X.loc[index, f]  = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_X.dropna(inplace=True)\n",
    "X_train = All_X.loc[All_X['Id']<list(np.where(All_X['Id_bat'] != All_X['Id_bat'].shift(1)))[0][4]].copy() #to update Y_train\n",
    "X_test = All_X.loc[All_X['Id']>=list(np.where(All_X['Id_bat'] != All_X['Id_bat'].shift(1)))[0][4]].copy()\n",
    "Y_train = All_Y_train[(All_Y_train['Id'].isin(X_train['Id'].values))].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got some missing values in our rolling attribute, an will try to address this issue using the MICE approach, as described [here](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3074241/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vertical splitting\n",
    "\n",
    "Missing data in Y : Some data is missing for Bat 3 (i.e. y1) but should be here. We don't want to train on those observations for Y1. So we decide here that we shall separate the prediction of each variable and get rid of only the minimum of observations for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X :\n",
      "(11676, 35)\n",
      "Y :\n",
      "(11676, 6)\n"
     ]
    }
   ],
   "source": [
    "#print(All_X_train.describe(), All_Y_train.describe())\n",
    "print('X :')\n",
    "print(X_train.shape)\n",
    "print('Y :')\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_div = {}\n",
    "X_div = {}\n",
    "X_test_div = {}\n",
    "\n",
    "### x train\n",
    "X_div['y1'] = X_train[X_train['Id_bat'].isin([1,2,3,4])].copy()\n",
    "X_div['y5'] = X_train[X_train['Id_bat'].isin([1,2,3,4])].copy()\n",
    "\n",
    "X_div['y2']  = X_train[X_train['Id_bat'].isin([1,2,4])].copy()\n",
    "X_div['y4']  = X_train[X_train['Id_bat'].isin([1,2,4])].copy()\n",
    "\n",
    "X_div['y3']  = X_train[X_train['Id_bat'].isin([3])].copy()\n",
    "\n",
    "### x test\n",
    "X_test_div['y1'] = X_test[X_test['Id_bat'].isin([1,2,3,4])].copy()\n",
    "X_test_div['y5'] = X_test[X_test['Id_bat'].isin([1,2,3,4])].copy()\n",
    "\n",
    "X_test_div['y2']  = X_test[X_test['Id_bat'].isin([1,2,4])].copy()\n",
    "X_test_div['y4']  = X_test[X_test['Id_bat'].isin([1,2,4])].copy()\n",
    "\n",
    "X_test_div['y3']  = X_test[X_test['Id_bat'].isin([3])].copy()\n",
    "\n",
    "### y train\n",
    "\n",
    "Y_div['y1'] = Y_train[X_train['Id_bat'].isin([1,2,3,4])][['Id',\"y1\"]].copy()\n",
    "Y_div['y5'] = Y_train[X_train['Id_bat'].isin([1,2,3,4])][['Id',\"y5\"]].copy()\n",
    "\n",
    "Y_div['y2'] = Y_train[X_train['Id_bat'].isin([1,2,4])][['Id',\"y2\"]].copy()\n",
    "Y_div['y4'] = Y_train[X_train['Id_bat'].isin([1,2,4])][['Id',\"y4\"]].copy()\n",
    "\n",
    "Y_div['y3'] = Y_train[X_train['Id_bat'].isin([3])][['Id',\"y3\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2429, 35)\n",
      "(1784, 35)\n",
      "(645, 35)\n",
      "(1784, 35)\n",
      "(2429, 35)\n"
     ]
    }
   ],
   "source": [
    "factors = ['y1','y2','y3','y4','y5']\n",
    "\n",
    "for f in factors:\n",
    "    print(X_test_div[f].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove the last missing value, this time from Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10449, 35)\n",
      "(10449, 2)\n",
      "(7917, 35)\n",
      "(7917, 2)\n",
      "(2861, 35)\n",
      "(2861, 2)\n",
      "(6780, 35)\n",
      "(6780, 2)\n",
      "(9352, 35)\n",
      "(9352, 2)\n"
     ]
    }
   ],
   "source": [
    "factors = ['y1','y2','y3','y4','y5']\n",
    "for f in factors:\n",
    "    Y_div[f].dropna(inplace=True)\n",
    "    X_div[f] = X_div[f][X_div[f]['Id'].isin(Y_div[f]['Id'])].copy()\n",
    "    Y_div[f].drop('Id',axis=1)\n",
    "    print(X_div[f].shape)\n",
    "    print(Y_div[f].shape) #OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the data to train later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "output = open('Div_data.pkl','wb')\n",
    "pickle.dump({'X_div':X_div, 'Y_div':Y_div,'X_test_div':X_test_div},output,-1)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning with cross-validation\n",
    "We want to use five different models for each of our y variable. We generate them using the same template, and will use the pipelines of sklearn to faciliate our work. \n",
    "We shall use the RandomForest algo, and use cross-validation (shuffleSplit is enough as the data are approximately well distributed between buildings) to evaluate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "n_estimators = 50\n",
    "def give_me_a_model(style = \"RF\", params = [n_estimators]):\n",
    "    if style == \"MLP\":\n",
    "        m = MLPRegressor((150,150,150,150),solver ='lbfgs',\n",
    "            batch_size = 200,max_iter = 100, verbose = True, early_stopping =False)\n",
    "    elif style == \"RF\" :\n",
    "        m = RandomForestRegressor(n_estimators = n_estimators,n_jobs = -1, verbose=0)\n",
    "    return(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Y_div['y1'].iloc[:,1:].head(2)\n",
    "#X_div['y1'].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model for y1 has been cross-validated. The 5-MSE is : -366.118555599\n",
      "Model for y2 has been cross-validated. The 5-MSE is : -1471.00662283\n",
      "Model for y3 has been cross-validated. The 5-MSE is : -34.491204761\n",
      "Model for y4 has been cross-validated. The 5-MSE is : -686.204179823\n",
      "Model for y5 has been cross-validated. The 5-MSE is : -1919.73896433\n",
      "22387.7976368\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import ShuffleSplit # we don't really need to stratify by building.\n",
    "style = \"RF\"\n",
    "m = {}\n",
    "scores = {}\n",
    "factors = ['y1','y2','y3','y4','y5']\n",
    "n_splits = 5\n",
    "for f in factors:  \n",
    "    m[f] = give_me_a_model(style)\n",
    "    skf = ShuffleSplit(n_splits=n_splits)\n",
    "    scores[f] = cross_validate(m[f], X_div[f].iloc[:,4:], np.ravel(Y_div[f].iloc[:,1:]), \n",
    "                               scoring = 'neg_mean_squared_error',cv = skf) #, return_train_score = True)\n",
    "    print('Model for '+f+' has been cross-validated. The '+str(n_splits)+'-MSE is : '+str(-np.mean(scores[f]['test_score'])))\n",
    "score = 0\n",
    "for f in factors:\n",
    "    score+= np.mean(scores[f]['test_score'])\n",
    "print(-score*n_splits) #19534"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for f in factors:  \n",
    "    m[f] = give_me_a_model(style)\n",
    "    m[f].fit(X_div[f].iloc[:,4:], np.ravel(Y_div[f].iloc[:,1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning using an off-the-shelve optimizer and RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "params = {\"n_estimators\" : list(range(25,150,15))+[175,200]}\n",
    "par = {}\n",
    "style = \"RF\"\n",
    "m = {}\n",
    "factors = ['y1','y2','y3','y4','y5']\n",
    "for f in factors:  \n",
    "    rfg = RandomForestRegressor(n_jobs=-1)\n",
    "    m[f] = RandomizedSearchCV(rfg, params, n_jobs=1,refit=True)  \n",
    "    par[f] = m[f].best_params\n",
    "    m[f].fit(X_div[f].iloc[:,4:], np.ravel(Y_div[f].iloc[:,1:]))  \n",
    "    print('Best params found '+f+' with randomized search' + str(par[f]))\n",
    "    print('Model for '+f+' was fitted and is ready to use. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-e1ce74716beb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mXGBRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m params = {  \n\u001b[0;32m      5\u001b[0m         \u001b[1;34m\"n_estimators\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m40\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost.sklearn import XGBClassifier  \n",
    "from xgboost.sklearn import XGBRegressor\n",
    "\n",
    "#from sklearn.metrics import mean_squared_error\n",
    "\n",
    "style = \"XGBRegressor\"\n",
    "m = {}\n",
    "factors = ['y1','y2','y3','y4','y5']\n",
    "for f in factors:  \n",
    "    m[f] = XGBRegressor(nthreads=-1)\n",
    "    \n",
    "    one_to_left = st.beta(10, 1)  \n",
    "    from_zero_positive = st.expon(0, 50)\n",
    "    params = {  \n",
    "        \"n_estimators\": st.randint(3, 40),\n",
    "        \"max_depth\": st.randint(3, 40),\n",
    "        \"learning_rate\": st.uniform(0.05, 0.4),\n",
    "        \"colsample_bytree\": one_to_left,\n",
    "        \"subsample\": one_to_left,\n",
    "        \"gamma\": st.uniform(0, 10),\n",
    "        'reg_alpha': from_zero_positive,\n",
    "        \"min_child_weight\": from_zero_positive,\n",
    "    }\n",
    "    \n",
    "    gs = RandomizedSearchCV(m[f], params, n_jobs=1)  \n",
    "    gs.fit(X_div[f].iloc[:,4:], np.ravel(Y_div[f].iloc[:,1:]))\n",
    "    m[f] = gs.best_model_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction, shaping and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = {}\n",
    "for f in factors:\n",
    "    y = pd.DataFrame(m[f].predict(X_test_div[f].iloc[:,4:]))\n",
    "    y['Id'] = X_test_div[f]['Id'].values\n",
    "    y = y.set_index('Id')\n",
    "    y_pred[f] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_y = pd.concat(y_pred, axis=1)\n",
    "predicted_y.columns = factors\n",
    "predicted_y.insert(0,'Id',predicted_y.index.tolist())\n",
    "predicted_y.fillna(0,inplace=True)\n",
    "print(predicted_y.head(5), predicted_y.shape) #OK\n",
    "\n",
    "predicted_y.to_csv(path_or_buf='RF_with_opti.csv',sep=';',header = True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_learning_curve(m24, title, X_train_24, Y_train_24, cv=cv)\n",
    "title = \"Learning Curves)\"\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_learning_curve(m3, title, X_train_3, Y_train_3, cv=cv)\n",
    "title = \"Learning Curves (SVM, RBF kernel, $\\gamma=0.001$)\"\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate the prediction for the test set and save the result, before uploading it to the competition's website."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
